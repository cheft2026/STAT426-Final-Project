# STAT426-Final-Project
### KNN Classification	
K-Nearest Neighbors, or KNN, is the first classification method we will use. KNN is a classification method in which an observation is classified based on the majority class of the K nearest observations. We train the model using both the flattened data and the PCA data. Before we can start training the data, we need to find the best K value as to not overfit or undefit the data. This is done through Cross Validation, using the Python command skm.GridSearchCV which employs a k-fold Cross Validation. We will be using a 5-fold Cross Validation due to the training sets having 1000 observations, thus it will be computationally advantageous to divide those up into 5 groups. Of the K values tested, K = 1 ended up being the best value for both training sets, meaning the decision boundary needs to be highly flexible to fit the data. Thus, we can set n_neighbors = 1 and use the KNeighborsClassifier() function on the training sets. Upon using knn.predict() on the testing sets and using the function acc_table to find the accuracies, the total accuracy for the flattened data and the PCA data was 75.06% and 75.59% respectively. While the increased accuracy of the PCA data is almost negligible, the reduced dimensions of the PCA data allows for a quicker computation, which will be important for more intensive models. Since we know the PCA data will be more accurate, we will use it from now on when testing the other models.

### Support Vector Machines
The next classification method we will use is the Support Vector Machines, or SVM. We will use both a linear kernel and a radial (nonlinear) kernel. First, we will start with a linear SVM. Similarly to KNN, we will need to perform a Cross Validation in order to determine the C value. In this case, the C value dictates the level of misclassification allowed in order to find the optimal hyperplane. The higher the C value, the larger the margin hyperplane, which is useful for data with outliers. Using 5-fold Cross Validation, we find that C = 0.01 is the best C value. This indicates that the data uses a small-margin hyperplane. Therefore, the data has low bias but high variance. With C = 0.01, we can train the model off the PCA training set using the SupportVectorClassifer() function and make a prediction using the testing data. We also need to make sure the kernel = ‘linear’. Using acc_table gives a total accuracy of 79.59%.
In situations where the clusters of data are unable to be linearly separated, a Nonlinear kernel can be used. Here, we use the Radial Basis Function, or RBF, as our kernel because it is infinitely dimensional. This is useful since our PCA data has a dimension of 69. RBF is a function of the squared distance between two observations, which shows the given influence the observations have on each other. The coefficient gamma scales this distance. The higher the gamma value, the smaller the influence. In addition to finding the best C value, we will also need to Cross Validate to find the best gamma value. Setting the kernel = ‘rbf’ and running the gridsearch shows that the best values are C = 10 and gamma = 0.001. In a Nonlinear setting, the data requires more misclassifications to set the hyperplane and a more loose classification criteria for each point. After training the model with the PCA data and fitting the testing data, the total accuracy given is 80.74%. Comparing this to the Linear SVM shows that our data requires a Nonlinear decision boundary in order to have more accurate classifications. 

### Random Forest
We choose to use Random Forests over other Decision Tree models because of its ability to deprioritize strong predictors, thus decorrelating the predictors. This is useful as it will reduce the variance of our dataset. We can use the RandomForestClassifier() function in python to do this. For our forest, we will generate B = 500 trees which will be sufficiently large enough as to not underfit our data. The maximum number of features (m) for each split will be the square root of the number of features (p) of the PCA training set, which is the standard for Random Forests. Since there are no hyperparameters, we do not need to perform a Cross Validation. In the code, we can generate the OOB score and the confusion matrix to further analyze how well this method performs. Upon training the model and testing it using the PCA testing set, we are given this confusion matrix.
  
![alt text](https://github.com/cheft2026/STAT426-Final-Project/blob/main/stat426fig1.png?raw=true)

The confusion matrix shows the number of observations from class j that were classified as class i. From the confusion matrix, we can see that the model was most likely to misclassify class 6 as class 0, those being Shirt and Tshirt/Top respectively. This corresponds with the acc_table findings, which shows that only 46.50% of Shirts were correctly classified. This can also be seen with the large number of class 2 (Pullovers) observations being classified as class 4 (Coats). The OOB (Out of Bag) score is a method of estimating the testing accuracy of decision tree models. From the acc_table, the total accuracy is 78.94%. This differs from the OOB score which is 81%, which shows that the estimation is less accurate than the results. While the accuracy of the Random Forests is high, it clearly struggles with differentiating between classes that could be considered “similar”, leading to a slight decrease in total accuracy.

### Linear Discriminant Analysis and Quadratic Discriminant Analysis
Linear Discriminant Analysis (LDA) is a classification method that reduces the dimensions of the data by focusing on maximizing the separability among known categories. This is useful for our PCA data, which as established has 69 dimensions. In practice, LDA creates a linear decision boundary in order to separate the classes. Recreating this in Python is simple, as all that needs to be done is training the PCA data using lda.fit() and then using that to make predictions on the testing set. Using acc_table, we find the total accuracy to be 77.50%. Similarly to the linear SVM, the data is capable of being linearly separated, but it is not as accurate as other models that prioritize variance.
Quadratic Discriminant Analysis (QDA) is similar to LDA in that it reduces the dimensions of the data in order to create separability. However, QDA differs from LDA because the decision boundary that is created is Nonlinear, usually in the form of a quadratic. This allows for more accurate results for data with a higher variance. However, when training the data using qda.fit() and predicting the testing data, we find the total accuracy using QDA is lower than LDA. From the acc_table, the total accuracy is 71.89%. This could be from the fact that class 0 and class 7 do not have full rank covariance matrices, which causes an error within the coding. Taken at face value, it would seem the data benefits from the underfitting LDA line compared to the overfitting QDA line.

### Comparison of Non-DL Models

![alt text](https://github.com/cheft2026/STAT426-Final-Project/blob/main/stat426fig2.png?raw=true)
	
  Above is the total test accuracies of each of the models. Nonlinear SVM trained on the PCA data scored the highest, whereas QDA trained on the PCA data scored the lowest. In fact, it appears both versions of the SVM are more favorable when using the PCA data. 

![alt text](https://github.com/cheft2026/STAT426-Final-Project/blob/main/stat426fig1.png?raw=true)

![alt text](https://github.com/cheft2026/STAT426-Final-Project/blob/main/stat426fig1.png?raw=true)

![alt text](https://github.com/cheft2026/STAT426-Final-Project/blob/main/stat426fig1.png?raw=true)

The tables above show the individual breakdowns of the testing accuracies for each class. The easiest to predict classes for each model are class 1 (Trouser), class 8 (Bag) and class 9 (Ankle Boot). All of these classes scored above 90% for most models. On average, class 6 has the lowest accuracy at approximately 51.7%. This follows from the confusion matrix that was made with the decision trees, which showed that class 6 was most likely to be classified as class 0. Since class 6 is Shirts and class 0 is Tshirts/Tops, it would make sense that the models would struggle between differentiating a “Shirt” and a “Tshirt/Top”. This could be a result from the PCA data reducing dimensions, possibly eliminating the features that made the two classes more distinguishable. 
Of the models, Nonlinear SVMs perform the best overall. While some models outperform Nonlinear SVMs for some classes, Nonlinear SVMs tend to be consistently more accurate. Random Forests can more accurately classify class 0 and class 9 than Nonlinear SVMs, but Nonlinear SVMs are more accurate for the rest. LDA has the highest testing accuracy for class 6 of all the models (51.7%), but compared to Nonlinear SVMs it performs significantly worse, especially when classifying class 2 (another class which was commonly misclassified). The training set given is highly variable and, even with applying PCA, still has 69 features. As such, opting for a training model that succeeds with highly variable data in higher dimensions is needed. Thus, Nonlinear SVMs would be the most appropriate choice, which the data shows.

### Deep Neural Networks
Now that we have analyzed the results of 6 different models, we are going to utilize a method of Deep Learning known as Convolutional Neural Networks (CNN). CNNs are able to mimic the pattern-seeking behaviors of humans in order to classify objects through the use of convolution layers and pooling layers. Convolution layers identify traits within the image and assign values to them, whereas pooling layers reduce the size of images into “summary images”.
First we will use a smaller version of the unflattened training set with only 1000 samples. We load the training set and the testing set using torch.utils.data.Dataloader into trainloader and testloader. Then we build a model consisting of multiple convolution and pooling layers and run a command in order to see the summary. In the summary, we see that there are 206,922 trainable parameters. The output layer starts with 16 channels in dimension 28 x 28. The dimension is halved when entering both pooling layers, leaving it with 32 channels in dimension 7 x 7. Then it is flattened, causing the 7 x 7 matrix to be transformed into a vector of size 1568. This is then eventually reduced into being a vector of size 10. From there we use the nn.CrossEntropyLoss function in order to compute the cross entropy loss between input logits and the target. We create a For loop that loops the dataset for 50 epochs and calculates the loss. Finally, we create another For loop that loads in the images from testloader and runs them through our model, saving the predicted labels in pred_labels and their true labels into true_labels. We use acc_table to find the total accuracy to be 82.75%, which is an improvement over the previous models, but only about 2% when compared to Nonlinear SVMs.
We will now try using the full training set. The same steps are performed, except we now loop the entropy loss for 10 epochs instead of 50 epochs due to the longer computational time required for the full data set. When using the full data set, the total accuracy is 91.10%. There is approximately an 8% increase in accuracy from when we only used 1000 samples, showing that using more samples when running a CNN will give more accurate results. 

### Conclusion
Through the use of different models, we can come to various conclusions about the Fashion MNIST dataset as well as how the models function themselves. Starting with KNN showed that using the PCA training data for the rest of the samples would be beneficial since the reduction of dimensions allowed for more accurate results and quicker computations. From there, the testing accuracies of each model varied based on how it dealt with the high dimension of the data and the high variance. The model that scored the highest total accuracy that was not a Deep Learning model was Nonlinear SVMs. This is because Nonlinear SVMs can account for overlapping data and data that can not be linearly separated, both being indicators of highly varied data. With CNNs, the total accuracy increased as we increased the amount of samples in the training data to the full amount. Therefore, the best models to use for this data would be Nonlinear SVMs and CNN with 1000 samples for computational efficiency, or CNN with the full amount of samples for accurate data. 
